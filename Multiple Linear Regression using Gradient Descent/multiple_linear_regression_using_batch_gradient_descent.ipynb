{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93316f89",
   "metadata": {},
   "source": [
    "### üß† Problem with OLS when Data or Features are Large\n",
    "\n",
    "OLS (Ordinary Least Squares) estimates coefficients using:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = (X^TX)^{-1}X^Ty\n",
    "$$\n",
    "\n",
    "It minimizes the cost function:\n",
    "\n",
    "$$\n",
    "J(\\beta) = \\frac{1}{2m}\\sum_{i=1}^{m}(y_i - X_i\\beta)^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Problems when Data or Columns Increase\n",
    "\n",
    "1. **High Computational Cost:**  \n",
    "   The matrix inversion \\( (X^TX)^{-1} \\) has a time complexity of \\( O(n^3) \\).  \n",
    "   When the number of features \\( n \\) is large, OLS becomes very slow.\n",
    "\n",
    "2. **High Memory Usage:**  \n",
    "   \\( X^TX \\) is an \\( n \\times n \\) matrix.  \n",
    "   Large \\( n \\) means huge memory consumption.\n",
    "\n",
    "3. **Multicollinearity:**  \n",
    "   If columns (features) are correlated, \\( X^TX \\) becomes nearly singular.  \n",
    "   Its inverse \\( (X^TX)^{-1} \\) may not exist or gives unstable results.\n",
    "\n",
    "4. **Overfitting:**  \n",
    "   With too many features compared to observations,  \n",
    "   OLS can fit noise instead of the actual pattern.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Hence:**  \n",
    "For large datasets or high-dimensional data,  \n",
    "methods like **Gradient Descent** are preferred over pure OLS.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3bd638",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Gradient Descent for Multiple Linear Regression\n",
    "\n",
    "**Multiple Linear Regression** predicts the target variable \\( y \\) using multiple input features:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\dots + \\theta_nx_n\n",
    "$$\n",
    "\n",
    "or in vector form:\n",
    "\n",
    "$$\n",
    "\\hat{y} = X\\theta\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- \\( X \\) ‚Üí feature matrix of size \\( (m \\times n) \\)  \n",
    "- \\( \\theta \\) ‚Üí parameter vector (weights)  \n",
    "- \\( m \\) ‚Üí number of samples  \n",
    "- \\( n \\) ‚Üí number of features  \n",
    "\n",
    "---\n",
    "\n",
    "### üìâ Cost Function\n",
    "\n",
    "The goal is to minimize the **Mean Squared Error (MSE)**:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (y_i - \\hat{y_i})^2\n",
    "$$\n",
    "\n",
    "or in matrix form:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m}(y - X\\theta)^T(y - X\\theta)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ Gradient Descent Update Rule\n",
    "\n",
    "At each iteration, the parameters are updated as:\n",
    "\n",
    "$$\n",
    "\\theta := \\theta - \\alpha \\frac{\\partial J}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "where the gradient of the cost function is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\theta} = \\frac{-2}{m} X^T(y - X\\theta)\n",
    "$$\n",
    "\n",
    "So the update rule becomes:\n",
    "\n",
    "$$\n",
    "\\theta := \\theta - \\frac{\\alpha}{m} X^T(X\\theta - y)\n",
    "$$\n",
    "\n",
    "and the bias term (intercept) is updated as:\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\frac{-2}{m}\\sum(y - X\\theta)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö° Advantages over OLS\n",
    "\n",
    "1. **Efficient for large feature sets:**  \n",
    "   Does not require computing \\( (X^TX)^{-1} \\), which is expensive when \\( n \\) is large.\n",
    "\n",
    "2. **Scalable and memory-friendly:**  \n",
    "   Works iteratively ‚Äî suitable for high-dimensional data and large datasets.\n",
    "\n",
    "3. **Flexible:**  \n",
    "   Works for **Batch**, **Mini-batch**, and **Stochastic** variants.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Limitations\n",
    "\n",
    "1. Requires **feature scaling** for faster convergence.  \n",
    "2. Choosing a proper **learning rate (\\( \\alpha \\))** is critical:\n",
    "   - Too high ‚Üí Divergence  \n",
    "   - Too low ‚Üí Slow learning  \n",
    "3. Needs multiple iterations to converge compared to direct OLS.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary\n",
    "\n",
    "Gradient Descent in **Multiple Linear Regression** optimizes all coefficients simultaneously by iteratively reducing the cost function.  \n",
    "It‚Äôs the preferred method when the dataset has **many features** or **is too large** for OLS to handle efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "fe2d8015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ad0289bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Student_Performance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "fc04a3f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hours Studied</th>\n",
       "      <th>Previous Scores</th>\n",
       "      <th>Extracurricular Activities</th>\n",
       "      <th>Sleep Hours</th>\n",
       "      <th>Sample Question Papers Practiced</th>\n",
       "      <th>Performance Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>99</td>\n",
       "      <td>Yes</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>82</td>\n",
       "      <td>No</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>51</td>\n",
       "      <td>Yes</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>52</td>\n",
       "      <td>Yes</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>75</td>\n",
       "      <td>No</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Hours Studied  ...  Performance Index\n",
       "0              7  ...               91.0\n",
       "1              4  ...               65.0\n",
       "2              8  ...               45.0\n",
       "3              5  ...               36.0\n",
       "4              7  ...               66.0\n",
       "\n",
       "[5 rows x 6 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f3f16fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Extracurricular Activities'] = data['Extracurricular Activities'].apply(lambda x: 1 if x == 'Yes' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "49d61dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, 1:5].values\n",
    "Y = data['Performance Index'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "bb9bd32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47f4d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "1bf92aa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8200137849144006"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GradientDescent:\n",
    "    def __init__(self):\n",
    "        self.coef = 0\n",
    "        self.intercept = 0\n",
    "    def fit(self, x_train, y_train, learning_rate = 0.0001, epoches = 10000):\n",
    "        self.coef = np.ones(x_train.shape[1])\n",
    "        \n",
    "        for i in range(epoches):\n",
    "            y_pred = np.dot(x_train, self.coef) + self.intercept\n",
    "            \n",
    "            new_b = (-2 / len(x_train)) * np.sum(y_train - y_pred)\n",
    "            self.intercept = self.intercept - (learning_rate * new_b)\n",
    "            \n",
    "            new_m = (-2 / len(x_train)) * (np.dot((y_train - y_pred), x_train))\n",
    "            self.coef = self.coef - (learning_rate * new_m)    \n",
    "            \n",
    "    def predict(self, x_test):\n",
    "        return np.dot(x_test, self.coef) + self.intercept\n",
    "g = GradientDescent()\n",
    "g.fit(x_train, y_train)\n",
    "r2_score(y_test, g.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "bbbeac79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>R2 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67.970772</td>\n",
       "      <td>0.820014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         MSE  R2 Score\n",
       "0  67.970772  0.820014"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\"MSE\":[mean_squared_error(y_test, g.predict(x_test))],\n",
    "              \"R2 Score\":[r2_score(y_test, g.predict(x_test))]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022a2f72",
   "metadata": {},
   "source": [
    "### üìä Model Evaluation Summary\n",
    "\n",
    "After training the model using **Batch Gradient Descent**, the results obtained are:\n",
    "\n",
    "- **Mean Squared Error (MSE):** 67.970772  \n",
    "- **R¬≤ Score:** 0.820014  \n",
    "\n",
    "These results indicate that the model performs reasonably well ‚Äî  \n",
    "an \\( R^2 \\) score of 0.82 means that about **82% of the variance** in the target variable is explained by the model.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Can it be Improved?\n",
    "\n",
    "Yes, the performance **can be improved further** by:\n",
    "- Increasing the number of epochs.  \n",
    "- Tuning the learning rate.  \n",
    "- Applying feature scaling or normalization more effectively.  \n",
    "- Using regularization techniques like Ridge or Lasso Regression.\n",
    "\n",
    "However, in this case, **the objective was not to build a high-accuracy project**,  \n",
    "but to **implement and understand the Gradient Descent algorithm** for Multiple Linear Regression from scratch.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Conclusion:**  \n",
    "The algorithm works correctly and achieves a good performance level,  \n",
    "which successfully demonstrates the concept and working of **Gradient Descent** in **Multiple Linear Regression**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
