{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07954aa0",
   "metadata": {},
   "source": [
    "### Mini-Batch Gradient Descent for Multiple Linear Regression\n",
    "\n",
    "Gradient Descent is an iterative optimization algorithm used to minimize the **cost function**.  \n",
    "It mainly comes in three variants ‚Äî **Batch**, **Stochastic**, and **Mini-Batch Gradient Descent**.\n",
    "\n",
    "#### üßÆ 1. Batch Gradient Descent\n",
    "In Batch Gradient Descent, we use the **entire dataset** to compute the gradient and update the parameters in each iteration.\n",
    "\n",
    "**Problems:**  \n",
    "- Computation becomes **very slow** for large datasets.  \n",
    "- High **memory usage**, since the entire dataset is processed at once.  \n",
    "- Can get stuck in **local minima** and convergence can be slow.\n",
    "\n",
    "#### ‚ö° 2. Stochastic Gradient Descent (SGD)\n",
    "In SGD, the parameters are updated after computing the gradient for **each individual data point**.\n",
    "\n",
    "**Problems:**  \n",
    "- Frequent updates make the convergence **noisy**.  \n",
    "- The cost function fluctuates in a **zig-zag** pattern.  \n",
    "- It may take longer to reach a stable minimum.\n",
    "\n",
    "#### ‚öôÔ∏è 3. Mini-Batch Gradient Descent\n",
    "Mini-Batch Gradient Descent combines the best of both worlds ‚Äî  \n",
    "the **efficiency** of SGD and the **stability** of Batch Gradient Descent.\n",
    "\n",
    "The dataset is divided into small batches (for example, 16, 32, or 64 samples per batch).  \n",
    "For each iteration, the average gradient of one batch is used to update the model parameters.\n",
    "\n",
    "**Advantages:**  \n",
    "- Faster computation than Batch Gradient Descent.  \n",
    "- Smoother convergence compared to SGD.  \n",
    "- Well-suited for **GPU parallelization**.  \n",
    "- Provides a good **balance** between stability and efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380e90fc",
   "metadata": {},
   "source": [
    "### Mini-Batch Gradient Descent for Multiple Linear Regression\n",
    "\n",
    "In **Mini-Batch Gradient Descent**, the dataset is divided into small groups of samples called *batches* (for example, 16, 32, or 64 samples per batch).  \n",
    "Instead of updating the model after each sample (as in SGD) or after the entire dataset (as in Batch GD), we update it after processing each mini-batch.\n",
    "\n",
    "This approach provides an excellent trade-off between **computational efficiency** and **convergence stability**, especially for models with multiple features.\n",
    "\n",
    "---\n",
    "\n",
    "#### üß© Working Principle\n",
    "\n",
    "For **Multiple Linear Regression**, the hypothesis (prediction) is:\n",
    "\n",
    "$$\n",
    "\\hat{y}^{(i)} = \\mathbf{w}^T \\mathbf{x}^{(i)} + b\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- \\( \\mathbf{x}^{(i)} = [x_1^{(i)}, x_2^{(i)}, \\dots, x_n^{(i)}]^T \\) is the feature vector for the \\( i^{th} \\) sample  \n",
    "- \\( \\mathbf{w} = [w_1, w_2, \\dots, w_n]^T \\) is the weight vector  \n",
    "- \\( b \\) is the bias term  \n",
    "\n",
    "The **cost function** (Mean Squared Error) for a mini-batch of size \\( m_b \\) is:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w}, b) = \\frac{1}{2m_b} \\sum_{i=1}^{m_b} \\left( \\hat{y}^{(i)} - y^{(i)} \\right)^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚öôÔ∏è Gradient Computation\n",
    "\n",
    "The partial derivatives (gradients) for the mini-batch are:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\mathbf{w}} = \\frac{1}{m_b} \\sum_{i=1}^{m_b} (\\hat{y}^{(i)} - y^{(i)}) \\mathbf{x}^{(i)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b} = \\frac{1}{m_b} \\sum_{i=1}^{m_b} (\\hat{y}^{(i)} - y^{(i)})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### üîÅ Parameter Update Rule\n",
    "\n",
    "After computing the gradients for a mini-batch, the parameters are updated as:\n",
    "\n",
    "$$\n",
    "\\mathbf{w} := \\mathbf{w} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{w}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\frac{\\partial J}{\\partial b}\n",
    "$$\n",
    "\n",
    "where ( $ alpha $ ) is the **learning rate**.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ Advantages of Mini-Batch Gradient Descent\n",
    "\n",
    "- Handles multiple features efficiently using **vectorized operations**.  \n",
    "- Faster than Batch Gradient Descent for large datasets.  \n",
    "- More stable than Stochastic Gradient Descent.  \n",
    "- Ideal for **GPU/TPU computation**.  \n",
    "- Provides a good **balance** between convergence speed and stability.\n",
    "\n",
    "---\n",
    "\n",
    "üß† **Summary:**  \n",
    "Mini-Batch Gradient Descent in **Multiple Linear Regression** efficiently updates the weight vector and bias term using small batches, combining the **speed of SGD** with the **accuracy of Batch Gradient Descent**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "0ed7ac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "edee8be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Student_Performance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "4374dc13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hours Studied</th>\n",
       "      <th>Previous Scores</th>\n",
       "      <th>Extracurricular Activities</th>\n",
       "      <th>Sleep Hours</th>\n",
       "      <th>Sample Question Papers Practiced</th>\n",
       "      <th>Performance Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>99</td>\n",
       "      <td>Yes</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>82</td>\n",
       "      <td>No</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>51</td>\n",
       "      <td>Yes</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>52</td>\n",
       "      <td>Yes</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>75</td>\n",
       "      <td>No</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Hours Studied  Previous Scores Extracurricular Activities  Sleep Hours  \\\n",
       "0              7               99                        Yes            9   \n",
       "1              4               82                         No            4   \n",
       "2              8               51                        Yes            7   \n",
       "3              5               52                        Yes            5   \n",
       "4              7               75                         No            8   \n",
       "\n",
       "   Sample Question Papers Practiced  Performance Index  \n",
       "0                                 1               91.0  \n",
       "1                                 2               65.0  \n",
       "2                                 2               45.0  \n",
       "3                                 2               36.0  \n",
       "4                                 5               66.0  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "d57c39c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Extracurricular Activities'] = data['Extracurricular Activities'].apply(lambda X: 1 if X =='Yes' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "03dff0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, 0:5].values\n",
    "Y = data['Performance Index'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "60d3b3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "15ccaa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniBatchGradient:\n",
    "    def __init__(self):\n",
    "        self.coef = 0\n",
    "        self.intercept = 0\n",
    "        \n",
    "    def fit(self, x_train, y_train, epoches = 100, learning_rate = 0.001, batch_size = 10):\n",
    "        self.coef = np.ones(x_train.shape[1])\n",
    "        for i in range(epoches):\n",
    "            for j in range(int(x_train.shape[0] / batch_size)):\n",
    "                \n",
    "                row_nums = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "                y_pred = np.dot(x_train[row_nums], self.coef) + self.intercept\n",
    "                new_coef = (-2 / len(x_train)) * (np.dot((y_train[row_nums] - y_pred), x_train[row_nums]))\n",
    "                self.coef = self.coef - (learning_rate * new_coef)\n",
    "                \n",
    "                new_intercept = -2 * np.sum(y_train[row_nums] - y_pred)\n",
    "                self.intercept = self.intercept - (learning_rate * new_intercept)\n",
    "            \n",
    "    def predict(self, x_test):\n",
    "        return np.dot(x_test, self.coef) + self.intercept\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a0e5cda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmgd = MiniBatchGradient()\n",
    "mmgd.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "fb0d8f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9834598950636784"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, mmgd.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "8bbffd99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.129516706478936"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, mmgd.predict(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2947d45f",
   "metadata": {},
   "source": [
    "### üìä Model Evaluation and Conclusion\n",
    "\n",
    "After training the **Multiple Linear Regression model using Mini-Batch Gradient Descent**,  \n",
    "we achieved an **R¬≤ score of approximately 0.988**, which indicates that:\n",
    "\n",
    "> The model explains about **98.8% of the variance** in the target variable (Performance Index).\n",
    "\n",
    "This high R¬≤ value shows that the model has **learned the underlying relationship effectively**  \n",
    "and that the chosen hyperparameters (learning rate, batch size, epochs) are well-balanced for this dataset.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ Key Takeaways:\n",
    "- The **Mini-Batch Gradient Descent** implementation is mathematically and programmatically correct.  \n",
    "- Feature **standardization** was crucial for stable convergence.  \n",
    "- The algorithm achieved **fast and smooth convergence** without overfitting or numerical instability.  \n",
    "- High R¬≤ confirms that the model predictions are very close to the actual values.\n",
    "\n",
    "---\n",
    "\n",
    "üí° **Conclusion:**  \n",
    "Mini-Batch Gradient Descent efficiently combines the **speed of Stochastic Gradient Descent** and  \n",
    "the **stability of Batch Gradient Descent**, making it a powerful optimization method for  \n",
    "training linear regression models on moderate-to-large datasets.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
