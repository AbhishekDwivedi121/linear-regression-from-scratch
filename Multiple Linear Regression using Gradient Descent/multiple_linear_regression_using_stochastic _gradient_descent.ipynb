{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a28a1ea",
   "metadata": {},
   "source": [
    "## ðŸ§  Problems with Batch Gradient Descent\n",
    "\n",
    "Batch Gradient Descent (BGD) is conceptually simple and mathematically accurate,  \n",
    "but it has several **practical limitations** when dealing with large datasets or real-time learning.\n",
    "\n",
    "---\n",
    "\n",
    "### âš™ï¸ 1. High Computational Cost\n",
    "- In every epoch, BGD calculates the gradient using **all training samples**.\n",
    "- This requires performing a full pass through the entire dataset before a single update.\n",
    "- As the dataset grows, the time to compute each update increases drastically.\n",
    "\n",
    "ðŸ§® Example:\n",
    "If the dataset has 1 million rows,  \n",
    "one gradient update means processing all 1 million samples â†’ **very slow convergence**.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ’¾ 2. High Memory Usage\n",
    "- Since all data points are required at once,  \n",
    "  the entire dataset must fit into **memory (RAM)** during training.\n",
    "- For very large datasets, this becomes infeasible.\n",
    "\n",
    "âš ï¸ Real-world datasets (e.g., terabytes of data) cannot be loaded into memory completely.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ¢ 3. Slow Convergence\n",
    "- BGD updates weights **only once per epoch**,  \n",
    "  meaning it waits until all samples are processed before improving the model.\n",
    "- This leads to **delayed learning** â€” the model doesnâ€™t start improving until the full dataset is seen.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§± 4. Stuck in Local Minima (for Non-Convex Problems)\n",
    "- Because it uses the **exact average gradient**,  \n",
    "  the optimization path is very smooth and deterministic.\n",
    "- This can cause the model to get stuck in **local minima** or flat regions of the loss surface.\n",
    "\n",
    "---\n",
    "\n",
    "### âš¡ 5. Not Suitable for Online / Streaming Data\n",
    "- In real-time applications (like stock prediction, sensor data, or live user inputs),  \n",
    "  data arrives continuously.\n",
    "- BGD cannot handle such situations because it requires **all data upfront**  \n",
    "  before starting any update.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸš« Summary\n",
    "\n",
    "| Problem | Description |\n",
    "|----------|--------------|\n",
    "| **Computation** | Full dataset processed per update â†’ very slow |\n",
    "| **Memory** | Entire data must fit into memory |\n",
    "| **Convergence** | Weights updated only once per epoch |\n",
    "| **Local Minima** | Deterministic updates can get stuck |\n",
    "| **Online Learning** | Cannot adapt to streaming or incremental data |\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Therefore:\n",
    "> While Batch Gradient Descent is accurate for small datasets,  \n",
    "> it becomes inefficient and impractical for large-scale or real-time learning.  \n",
    "> This is why **Stochastic Gradient Descent (SGD)** and **Mini-Batch GD** are preferred in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4d95fb",
   "metadata": {},
   "source": [
    "---\n",
    "## âš¡ Stochastic Gradient Descent (SGD)\n",
    "\n",
    "### ðŸ§© Overview\n",
    "**Stochastic Gradient Descent (SGD)** is an optimization algorithm used to minimize a loss function and find the best model parameters.  \n",
    "Unlike **Batch Gradient Descent**, which updates parameters after processing the entire dataset,  \n",
    "SGD updates the parameters **after every single training sample**.  \n",
    "\n",
    "This makes it **faster** and **more memory-efficient**, especially for large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  How It Works (for Linear Regression)\n",
    "\n",
    "Prediction for a single sample:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = w x_i + b\n",
    "$$\n",
    "\n",
    "Error for that sample:\n",
    "\n",
    "$$\n",
    "e_i = y_i - \\hat{y}_i\n",
    "$$\n",
    "\n",
    "Loss for a single sample (Mean Squared Error):\n",
    "\n",
    "$$\n",
    "L_i = (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "Gradients with respect to parameters:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_i}{\\partial w} = -2\\, e_i\\, x_i,\n",
    "\\quad\n",
    "\\frac{\\partial L_i}{\\partial b} = -2\\, e_i\n",
    "$$\n",
    "\n",
    "Parameter update rules (using learning rate \\( \\eta \\)):\n",
    "\n",
    "$$\n",
    "w \\leftarrow w - \\eta \\frac{\\partial L_i}{\\partial w}\n",
    "\\quad \\Rightarrow \\quad\n",
    "w = w + 2\\eta\\, e_i\\, x_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "b \\leftarrow b - \\eta \\frac{\\partial L_i}{\\partial b}\n",
    "\\quad \\Rightarrow \\quad\n",
    "b = b + 2\\eta\\, e_i\n",
    "$$\n",
    "\n",
    "For multiple features (vector form):\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\mathbf{w}^\\top \\mathbf{x}_i + b\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{w}} L_i = -2\\, e_i\\, \\mathbf{x}_i,\n",
    "\\quad\n",
    "\\mathbf{w} \\leftarrow \\mathbf{w} + 2\\eta\\, e_i\\, \\mathbf{x}_i,\n",
    "\\quad\n",
    "b \\leftarrow b + 2\\eta\\, e_i\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ” Training Process (Conceptually)\n",
    "\n",
    "1. Initialize parameters \\( w, b \\)\n",
    "2. For each epoch:\n",
    "   - Randomly select one sample \\( (x_i, y_i) \\)\n",
    "   - Predict the output \\( \\hat{y}_i \\)\n",
    "   - Compute the error \\( e_i = y_i - \\hat{y}_i \\)\n",
    "   - Update \\( w \\) and \\( b \\) using the formulas above\n",
    "3. Repeat for all samples across multiple epochs until the loss converges\n",
    "\n",
    "---\n",
    "\n",
    "### âš™ï¸ Characteristics\n",
    "\n",
    "| Feature | Description |\n",
    "|----------|-------------|\n",
    "| **Update Frequency** | After every single sample |\n",
    "| **Gradient Accuracy** | Noisy / Approximate |\n",
    "| **Speed** | Very fast |\n",
    "| **Memory Requirement** | Very low |\n",
    "| **Convergence Behavior** | Fast but oscillatory |\n",
    "| **Local Minima Handling** | Can escape due to randomness |\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Advantages\n",
    "\n",
    "1. **Faster learning** for large datasets  \n",
    "2. **Online learning** â€” can update continuously as new data arrives  \n",
    "3. **Memory efficient** â€” no need to load all data into memory  \n",
    "4. **Escapes local minima** more easily due to random updates  \n",
    "\n",
    "---\n",
    "\n",
    "### âš ï¸ Limitations\n",
    "\n",
    "1. The convergence path is **noisy and unstable**  \n",
    "2. **Highly sensitive** to the learning rate  \n",
    "3. May **overshoot** near the global minimum if learning rate is too large  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸš€ Summary\n",
    "\n",
    "| Aspect | Batch Gradient Descent | Stochastic Gradient Descent |\n",
    "|---------|------------------------|------------------------------|\n",
    "| **Update per** | Full dataset | Single sample |\n",
    "| **Speed** | Slow | Fast |\n",
    "| **Memory Use** | High | Low |\n",
    "| **Convergence Path** | Smooth | Noisy |\n",
    "| **Large Dataset Handling** | Difficult | Excellent |\n",
    "| **Online Learning** | Not supported | Supported |\n",
    "\n",
    "---\n",
    "\n",
    "> âœ… **In short:**  \n",
    "> Stochastic Gradient Descent updates the model parameters after every individual sample,  \n",
    "> which leads to faster learning and better scalability â€” especially useful for very large or streaming datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "356d0e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfb8f303",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Student_Performance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cad8064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hours Studied</th>\n",
       "      <th>Previous Scores</th>\n",
       "      <th>Extracurricular Activities</th>\n",
       "      <th>Sleep Hours</th>\n",
       "      <th>Sample Question Papers Practiced</th>\n",
       "      <th>Performance Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>99</td>\n",
       "      <td>Yes</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>82</td>\n",
       "      <td>No</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>51</td>\n",
       "      <td>Yes</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>52</td>\n",
       "      <td>Yes</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>75</td>\n",
       "      <td>No</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Hours Studied  Previous Scores Extracurricular Activities  Sleep Hours  \\\n",
       "0              7               99                        Yes            9   \n",
       "1              4               82                         No            4   \n",
       "2              8               51                        Yes            7   \n",
       "3              5               52                        Yes            5   \n",
       "4              7               75                         No            8   \n",
       "\n",
       "   Sample Question Papers Practiced  Performance Index  \n",
       "0                                 1               91.0  \n",
       "1                                 2               65.0  \n",
       "2                                 2               45.0  \n",
       "3                                 2               36.0  \n",
       "4                                 5               66.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5522fee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Extracurricular Activities'] = data['Extracurricular Activities'].apply(lambda x: 1 if x == 'Yes' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1000a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, 0:5].values\n",
    "Y = data['Performance Index'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "751c0bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5798af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultipleStochastic_GradientDescent:\n",
    "    def __init__(self):\n",
    "        self.coef = 0\n",
    "        self.intercept = 0\n",
    "    def fit(self, x_train, y_train, learning_rate = 0.001, epoches = 50):\n",
    "        self.coef = np.ones(x_train.shape[1])\n",
    "        for i in range(epoches):\n",
    "            for j in range(x_train.shape[0]):\n",
    "                row_num = np.random.randint(0, x_train.shape[0])\n",
    "                y_pred = np.dot(self.coef, x_train[row_num]) + self.intercept\n",
    "                \n",
    "                \n",
    "                new_intercept = (-2 / len(x_train)) * (y_train[row_num] - y_pred)\n",
    "                self.intercept = self.intercept - (learning_rate * new_intercept)\n",
    "                \n",
    "                new_coef = (-2 / len(x_train)) * np.dot((y_train[row_num] - y_pred), x_train[row_num])\n",
    "                self.coef = self.coef - (learning_rate * new_coef)\n",
    "                \n",
    "    def predict(self, x_test):\n",
    "        return np.dot(x_test, self.coef) + self.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98d54b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = MultipleStochastic_GradientDescent()\n",
    "msg.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72c0cac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8621164045150476\n",
      "51.28872892028614\n"
     ]
    }
   ],
   "source": [
    "print(r2_score(y_test, msg.predict(x_test)))\n",
    "print(mean_squared_error(y_test, msg.predict(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c715ef50",
   "metadata": {},
   "source": [
    "## ðŸ“Š Model Evaluation and Observation\n",
    "\n",
    "After training the model using **Stochastic Gradient Descent**,  \n",
    "the evaluation metrics are as follows:\n",
    "\n",
    "| Metric | Value |\n",
    "|---------|--------|\n",
    "| RÂ² Score | 0.8621164045150476|\n",
    "| MSE | 51.28872892028614 |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Interpretation\n",
    "\n",
    "The **RÂ² score (0.86)** and **MSE (51.28)** indicate that the model  \n",
    "is not fitting the data very accurately yet.\n",
    "\n",
    "However, in this notebook, the **primary objective** was not to  \n",
    "achieve the best accuracy but to **implement the Stochastic Gradient Descent algorithm manually**  \n",
    "and understand how it works mathematically and programmatically.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Why Itâ€™s Not Improved Further\n",
    "\n",
    "The modelâ€™s performance could be improved by:\n",
    "- Adjusting the **learning rate**\n",
    "- Increasing the **number of epochs**\n",
    "- Applying **feature scaling / normalization**\n",
    "- Or using a **smaller step size**\n",
    "\n",
    "But these steps are intentionally **not applied here**,  \n",
    "because the main focus was to demonstrate the **algorithmic logic**  \n",
    "rather than optimization or model tuning.\n",
    "\n",
    "---\n",
    "\n",
    "âœ… **In short:**\n",
    "> This implementation focuses on understanding **how Stochastic Descent works**,  \n",
    "> not on achieving high accuracy or the best-fit line.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
